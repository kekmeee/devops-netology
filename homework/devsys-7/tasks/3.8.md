Домашняя работа по занятию "3.8. Компьютерные сети, лекция 3"
==

1. ipvs. Если при запросе на VIP сделать подряд несколько запросов
   (например, `for i in {1..50}; do curl -I -s 172.28.128.200>/dev/null; done `), ответы будут получены почти мгновенно.
   Тем не менее, в выводе `ipvsadm -Ln` еще некоторое время будут висеть активные `InActConn`. Почему так происходит?

<h3>Ответ:</h3>

В колонке `InActConn` находятся соединения, состояние которых отлично от `ESTABLISHED`. После отправки http-запросов
с помощью curl, нам возвращаются ответы через соединения, которые уже инициировал веб-сервер. После отправки ответов
все TCP-соединения перешли в состояние `TIME-WAIT` и в `ipvsadm -Ln` они из колонки `ActiveConn` перешли в `InActConn`.
В этой колонке они пробудут время, таймаут которого задается в `ipvsadm` (для просмотра можно ввести команду
`ipvsadm -L --timeout`)

---

2. На лекции мы познакомились отдельно с ipvs и отдельно с keepalived. Воспользовавшись этими знаниями, совместите
   технологии вместе (VIP должен подниматься демоном keepalived). Приложите конфигурационные файлы, которые у вас
   получились, и продемонстрируйте работу получившейся конструкции. Используйте для директора отдельный хост,
   не совмещая его с риалом! Подобная схема возможна, но выходит за рамки рассмотренного на лекции.

<h3>Ответ:</h3>
Состав и адреса хостов:
- Балансировщики:
  - 1stLoadBalancer:   IP - 172.28.128.10/24
  - 2ndLoadBalancer:   IP - 172.28.128.60/24 
- Backend серверы:
  - 1stWebServer:      IP - 172.28.128.110/24
  - 2ndWebServer:      IP - 172.28.128.160/24
- Клиент:
  - Client:            IP - 172.28.128.90/24

<h3>Настройка backend серверов</h3>

1. Поднимаем сетевой интерфейс:
````bash
root@{1st,2nd}WebServer:~# ip addr add 172.28.128.200/32 dev lo
````

2. Для удобства содержимое страницы приветствия nginx заменяем, на: ``Welcome to nginx {1st,2nd}WebServer ``
   
3. Меняем поведение при работе с ARP-запросами на backend серверах:
````bash
root@{1st,2nd}WebServer:~# sysctl -w net.ipv4.conf.all.arp_ignore=1 
net.ipv4.conf.all.arp_ignore = 1
root@{1st,2nd}WebServer:~# sysctl -w net.ipv4.conf.all.arp_announce=2
net.ipv4.conf.all.arp_announce = 2
````
 
<h3>Настройка балансировщиков</h3>

1. Устанавливаем ipvsadm и keepalived:
````bash
root@{1st,2nd}LoadBalancer:~# apt install -y ipvsadm keepalived
````

2. Создаем конфигурационный файл keepalived.conf и вносим конфигурацию:

На ``1stLoadBalancer``:
````bash
root@1stLoadBalancer:~# cat /etc/keepalived/keepalived.conf
vrrp_instance VI_1 {
    state MASTER
    interface eth1
    virtual_router_id 51
    priority 100
    advert_int 1
    authentication {
        auth_type PASS
        auth_pass kekmeee
    }
    virtual_ipaddress {
        172.28.128.200/32
    }
}
virtual_server 172.28.128.200 80 {
    delay_loop 10
    lvs_sched rr
    lvs_method DR
    protocol TCP

    real_server 172.28.128.110 80 {
        weight 1
        TCP_CHECK {
        connect_timeout 10
        connect_port 80
        }
    }
    real_server 172.28.128.160 80 {
        weight 1
        TCP_CHECK {
        connect_timeout 10
        connect_port 80
        }
     }
}
````
На ``2ndLoadBalancer``:
````bash
root@2ndLoadBalancer:~# cat /etc/keepalived/keepalived.conf
/etc/keepalived/keepalived.conf
vrrp_instance VI_1 {
    state BACKUP
    interface eth1
    virtual_router_id 51
    priority 50
    advert_int 1
    authentication {
        auth_type PASS
        auth_pass kekmeee
    }
    virtual_ipaddress {
        172.28.128.200/32
    }
}
virtual_server 172.28.128.200 80 {
    delay_loop 10
    lvs_sched rr
    lvs_method DR
    protocol TCP

    real_server 172.28.128.110 80 {
        weight 1
        TCP_CHECK {
        connect_timeout 10
        connect_port 80
        }
    }
    real_server 172.28.128.160 80 {
        weight 1
        TCP_CHECK {
        connect_timeout 10
        connect_port 80
        }
     }
}
````

3. Запускаем keepalived:
````bash
root@{1st,2nd}LoadBalancer:~# systemctl start keepalived
````

4. Проверка работоспособности балансировки:
   
   4.1. Проверка балансировки:
````bash
root@Client:~# curl 172.28.128.200
Welcome to nginx 2ndWebServer
root@Client:~# curl 172.28.128.200
Welcome to nginx 1stWebServer
root@Client:~# curl 172.28.128.200
Welcome to nginx 2ndWebServer
root@Client:~# curl 172.28.128.200
Welcome to nginx 1stWebServer
root@Client:~# for i in {1..99}; do curl -I -s 172.28.128.200>/dev/null; done

root@1stLoadBalancer:~# ipvsadm -Ln
IP Virtual Server version 1.2.1 (size=4096)
Prot LocalAddress:Port Scheduler Flags
  -> RemoteAddress:Port           Forward Weight ActiveConn InActConn
TCP  172.28.128.200:80 rr
  -> 172.28.128.110:80            Route   1      0          52
  -> 172.28.128.160:80            Route   1      0          51
````

5. Проверка отказоустойчивости:
  - Проверка health check:
На ``1stLoadBalancer``:
````bash
root@1stLoadBalancer:~# tcpdump -i eth1 -vvv
tcpdump: listening on eth1, link-type EN10MB (Ethernet), capture size 262144 bytes
14:21:07.305190 IP (tos 0xc0, ttl 255, id 943, offset 0, flags [none], proto VRRP (112), length 40) 1stLoadBalancer > vrrp.mcast.net: vrrp 1stLoadBalancer > vrrp.mcast.net: VRRPv2, Advertisement, vrid 51, prio 100, authtype simple, intvl 1s, length 20, addrs: 1stLoadBalancer auth "kekmeee^@"
14:21:08.306742 IP (tos 0xc0, ttl 255, id 944, offset 0, flags [none], proto VRRP (112), length 40) 1stLoadBalancer > vrrp.mcast.net: vrrp 1stLoadBalancer > vrrp.mcast.net: VRRPv2, Advertisement, vrid 51, prio 100, authtype simple, intvl 1s, length 20, addrs: 1stLoadBalancer auth "kekmeee^@"
14:21:09.308812 IP (tos 0xc0, ttl 255, id 945, offset 0, flags [none], proto VRRP (112), length 40) 1stLoadBalancer > vrrp.mcast.net: vrrp 1stLoadBalancer > vrrp.mcast.net: VRRPv2, Advertisement, vrid 51, prio 100, authtype simple, intvl 1s, length 20, addrs: 1stLoadBalancer auth "kekmeee^@"
````
На ``2ndLoadBalancer``:
````bash
root@2ndLoadBalancer:~# tcpdump -i eth1 -vvv
tcpdump: listening on eth1, link-type EN10MB (Ethernet), capture size 262144 bytes
14:21:07.317722 IP (tos 0xc0, ttl 255, id 943, offset 0, flags [none], proto VRRP (112), length 40) 172.28.128.10 > vrrp.mcast.net: vrrp 172.28.128.10 > vrrp.mcast.net: VRRPv2, Advertisement, vrid 51, prio 100, authtype simple, intvl 1s, length 20, addrs: 172.28.128.200 auth "kekmeee^@"
14:21:08.319055 IP (tos 0xc0, ttl 255, id 944, offset 0, flags [none], proto VRRP (112), length 40) 172.28.128.10 > vrrp.mcast.net: vrrp 172.28.128.10 > vrrp.mcast.net: VRRPv2, Advertisement, vrid 51, prio 100, authtype simple, intvl 1s, length 20, addrs: 172.28.128.200 auth "kekmeee^@"
14:21:09.321396 IP (tos 0xc0, ttl 255, id 945, offset 0, flags [none], proto VRRP (112), length 40) 172.28.128.10 > vrrp.mcast.net: vrrp 172.28.128.10 > vrrp.mcast.net: VRRPv2, Advertisement, vrid 51, prio 100, authtype simple, intvl 1s, length 20, addrs: 172.28.128.200 auth "kekmeee^@"
````

  - Отключаем первый балансировщик:
   ````bash
  root@1stLoadBalancer:~# systemctl stop keepalived
   ````
  - Проверяем второй балансировщик и убеждаемся, что VIP поднялся автоматически:
   ````bash
  root@2ndLoadBalancer:~# ip -4 addr show eth1 | grep inet
    inet 172.28.128.60/24 scope global eth1
    inet 172.28.128.200/24 scope global secondary eth1
   ````
  - отправляем запросы на backend сервера:
   ````bash
   root@Client:~# curl 172.28.128.200
   Welcome to nginx 1stWebServer
   root@Client:~# curl 172.28.128.200
   Welcome to nginx 2ndWebServer
   root@Client:~# curl 172.28.128.200
   Welcome to nginx 1stWebServer
   root@Client:~# curl 172.28.128.200
   Welcome to nginx 2ndWebServer
   root@Client:~# for i in {1..21}; do curl -I -s 172.28.128.200>/dev/null; done
   
   root@1stLoadBalancer:~# ipvsadm -Ln
   IP Virtual Server version 1.2.1 (size=4096)
   Prot LocalAddress:Port Scheduler Flags
     -> RemoteAddress:Port           Forward Weight ActiveConn InActConn
   TCP  172.28.128.200:80 rr
     -> 172.28.128.110:80            Route   1      0          13
     -> 172.28.128.160:80            Route   1      0          12
   ````
   - Проверяем health check:
На ``1stLoadBalancer``:
````bash
root@1stLoadBalancer:~# tcpdump -i eth1 -vvv
tcpdump: listening on eth1, link-type EN10MB (Ethernet), capture size 262144 bytes
14:28:27.301137 IP (tos 0xc0, ttl 255, id 103, offset 0, flags [none], proto VRRP (112), length 40) 172.28.128.60 > vrrp.mcast.net: vrrp 172.28.128.60 > vrrp.mcast.net: VRRPv2, Advertisement, vrid 51, prio 50, authtype simple, intvl 1s, length 20, addrs: 172.28.128.200 auth "kekmeee^@"
14:28:28.302545 IP (tos 0xc0, ttl 255, id 104, offset 0, flags [none], proto VRRP (112), length 40) 172.28.128.60 > vrrp.mcast.net: vrrp 172.28.128.60 > vrrp.mcast.net: VRRPv2, Advertisement, vrid 51, prio 50, authtype simple, intvl 1s, length 20, addrs: 172.28.128.200 auth "kekmeee^@"
14:28:29.303095 IP (tos 0xc0, ttl 255, id 105, offset 0, flags [none], proto VRRP (112), length 40) 172.28.128.60 > vrrp.mcast.net: vrrp 172.28.128.60 > vrrp.mcast.net: VRRPv2, Advertisement, vrid 51, prio 50, authtype simple, intvl 1s, length 20, addrs: 172.28.128.200 auth "kekmeee^@"
````
На ``2ndLoadBalancer``:
````bash
root@2ndLoadBalancer:~# tcpdump -i eth1 -vvv
tcpdump: listening on eth1, link-type EN10MB (Ethernet), capture size 262144 bytes
14:28:27.313257 IP (tos 0xc0, ttl 255, id 103, offset 0, flags [none], proto VRRP (112), length 40) 2ndLoadBalancer > vrrp.mcast.net: vrrp 2ndLoadBalancer > vrrp.mcast.net: VRRPv2, Advertisement, vrid 51, prio 50, authtype simple, intvl 1s, length 20, addrs: 2ndLoadBalancer auth "kekmeee^@"
14:28:28.314711 IP (tos 0xc0, ttl 255, id 104, offset 0, flags [none], proto VRRP (112), length 40) 2ndLoadBalancer > vrrp.mcast.net: vrrp 2ndLoadBalancer > vrrp.mcast.net: VRRPv2, Advertisement, vrid 51, prio 50, authtype simple, intvl 1s, length 20, addrs: 2ndLoadBalancer auth "kekmeee^@"
14:28:29.315266 IP (tos 0xc0, ttl 255, id 105, offset 0, flags [none], proto VRRP (112), length 40) 2ndLoadBalancer > vrrp.mcast.net: vrrp 2ndLoadBalancer > vrrp.mcast.net: VRRPv2, Advertisement, vrid 51, prio 50, authtype simple, intvl 1s, length 20, addrs: 2ndLoadBalancer auth "kekmeee^@"
````


---

3. В лекции мы использовали только 1 VIP адрес для балансировки. У такого подхода несколько отрицательных моментов,
   один из которых – невозможность активного использования нескольких хостов (1 адрес может только переехать с master
   на standby). Подумайте, сколько адресов оптимально использовать, если мы хотим без какой-либо деградации выдерживать
   потерю 1 из 3 хостов при входящем трафике 1.5 Гбит/с и физических линках хостов в 1 Гбит/с? Предполагается, что мы
   хотим задействовать 3 балансировщика в активном режиме (то есть не 2 адреса на 3 хоста, один из которых в обычное
   время простаивает).

<h3>Ответ:</h3>

Имеются три хоста:
- host1
- host2
- host3

Пускай на каждом хосте будет по два VIP адреса для балансировки нагрузки:
- host1: VIP1, VIP1.1
- host2: VIP2, VIP2.2
- host3: VIP3, VIP3.3

Тогда при выходе из строя одного из хостов каждый из его VIP адресов можно распределить между двумя остальными хостами:
````
|    HOST    |    Own VIP    |   Listen VIP     |
|-----------------------------------------------|
|    host1   |  VIP1,VIP1.1  |   VIP2,VIP3      | 
|-----------------------------------------------|
|    host2   |  VIP2,VIP2.2  |   VIP1,VIP3.3    | 
|-----------------------------------------------|
|    host3   |  VIP3,VIP3.3  |   VIP1.1,VIP2.2  | 
````
Оптимальное количество VIP адресов на каждом хосте равняется двум. В таком случаем каждый из хостов находится в
состоянии MASTER (отсутствует простой каждого из хостов), является резервным по отношению к каждому из
оставшихся (находится в состоянии BACKUP) и в случае выхода из строя одного из хостов нагрузка равномерно распределится
между оставшимися хостами.

---